{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow\n",
    "### tensorflow를 활용한 y = 2x + 1 선형회귀 모델 예측 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 준비\n",
    "# X _data : 입력 데이터, y_data: 실제 값 (정답)\n",
    "x_data = np.array([0, 1, 2, 3, 4], dtype=np.float32)\n",
    "y_data = np.array([1, 3, 5, 7, 9], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델 정의\n",
    "# 가중치(w)와 편향(b)를 변수로 정의\n",
    "w = tf.Variable(0.0) # 초기 가중치\n",
    "b = tf.Variable(0.0) # 초기 편향"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측함수 (y= wx + b)\n",
    "def predict(x):\n",
    "    return w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 손실 함수 정의\n",
    "# 평균 제곱 오차 ( Mean Squared Error, MSE) 사용\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred)) # reduce_mean : 제곱한 오차들의 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 최적화 알고리즘 선택\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01) # 확률적 경사 하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 33.0, w: 0.2800000011920929, b: 0.09999999403953552\n",
      "Step 10, Loss: 1.8755830526351929, w: 1.6574040651321411, b: 0.6004058718681335\n",
      "Step 20, Loss: 0.1229294165968895, w: 1.9802603721618652, b: 0.7298945188522339\n",
      "Step 30, Loss: 0.02239181473851204, w: 2.053251266479492, b: 0.7707916498184204\n",
      "Step 40, Loss: 0.014991003088653088, w: 2.0671944618225098, b: 0.7900874018669128\n",
      "Step 50, Loss: 0.013025244697928429, w: 2.0673303604125977, b: 0.8037028312683105\n",
      "Step 60, Loss: 0.011540031060576439, w: 2.06437611579895, b: 0.8154456615447998\n",
      "Step 70, Loss: 0.010236968286335468, w: 2.0608623027801514, b: 0.8262498378753662\n",
      "Step 80, Loss: 0.009081726893782616, w: 2.0573794841766357, b: 0.8363654017448425\n",
      "Step 90, Loss: 0.00805690512061119, w: 2.0540578365325928, b: 0.8458786010742188\n"
     ]
    }
   ],
   "source": [
    "# 5. 학습 과정\n",
    "for step in range(100): # 100번 반복\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = predict(x_data) # 예측값\n",
    "        loss = loss_fn(y_data, y_pred) # 손실계산\n",
    "        \n",
    "        gradients = tape.gradient(loss, [w, b])\n",
    "        optimizer.apply_gradients(zip(gradients, [w, b]))\n",
    "        \n",
    "        if step % 10 == 0: # 10번 마다 로그 출력\n",
    "            print(f\"Step {step}, Loss: {loss.numpy()}, w: {w.numpy()}, b: {b.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Parameters:w=2.051225423812866, b=0.8539638519287109\n",
      "prediction for x=5 11.110091\n"
     ]
    }
   ],
   "source": [
    "# 예측 및 결과 확인\n",
    "print(\"Final Parameters:\" f\"w={w.numpy()}, b={b.numpy()}\")\n",
    "print(\"prediction for x=5\", predict(5).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
